{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "def init_normal(shape=[0, 0.05], seed=None):\n",
    "    mean, stddev = shape\n",
    "    return keras.initializers.RandomNormal(\n",
    "        mean=mean, stddev=stddev, seed=seed)\n",
    "\n",
    "\n",
    "def normalize(tensor):\n",
    "    K.l2_normalize(tensor)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def get_model(num_users, num_items, num_tasks, e_dim=16, f_dim=8, reg=0):\n",
    "    \"\"\"\n",
    "    This function is used to get the Att-Mul-MF model described\n",
    "    in the paper.\n",
    "    Args:\n",
    "        :param num_users: number of users in the dataset\n",
    "        :param num_items: number of items in the dataset\n",
    "        :param num_tasks: number of tasks (item genres)\n",
    "        :param e_dim: the embedding dimension\n",
    "        :param f_dim: the preference feature space dimension\n",
    "        :param reg: regularization coefficient\n",
    "    \"\"\"\n",
    "    # Input variables\n",
    "    user_input = keras.layers.Input(shape=(1,), dtype='int32',\n",
    "                                    name='user_input')\n",
    "    item_input = keras.layers.Input(shape=(1,), dtype='int32',\n",
    "                                    name='item_input')\n",
    "\n",
    "    user_embedding = keras.layers.Embedding(\n",
    "        input_dim=num_users, output_dim=int(f_dim),\n",
    "        name='user_embedding',\n",
    "        embeddings_initializer=init_normal(),\n",
    "        embeddings_regularizer=keras.regularizers.l2(reg),\n",
    "        input_length=1)\n",
    "\n",
    "    item_embedding = keras.layers.Embedding(\n",
    "        input_dim=num_items, output_dim=int(e_dim),\n",
    "        name='item_embedding',\n",
    "        embeddings_initializer=init_normal(),\n",
    "        embeddings_regularizer=keras.regularizers.l2(reg),\n",
    "        input_length=1)\n",
    "\n",
    "    # Flatten the output tensor\n",
    "    user_latent = keras.layers.Flatten()(user_embedding(user_input))\n",
    "    item_latent = keras.layers.Flatten()(item_embedding(item_input))\n",
    "\n",
    "    # item vector feature extraction, split at the last layer\n",
    "    item_vector = keras.layers.Dense(\n",
    "        units=num_tasks*f_dim,\n",
    "        activation='relu',\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=keras.regularizers.l2(reg),\n",
    "        name='item_vector')(item_latent)\n",
    "    item_vector = keras.layers.Reshape(\n",
    "        (num_tasks, f_dim), name='multitask_vector')(item_vector)\n",
    "\n",
    "    weight_vector = keras.layers.Dot(axes=-1, normalize=True)(\n",
    "        [user_latent, item_vector])\n",
    "    att_vector = keras.layers.Dot(axes=(-1, -2))([weight_vector, item_vector])\n",
    "    att_vector = keras.layers.Reshape(\n",
    "        (f_dim,), name='attention_layer')(att_vector)\n",
    "\n",
    "    # Concatenate att_vector and gmf_layer\n",
    "    pred_vector = keras.layers.Concatenate()([user_latent, att_vector])\n",
    "\n",
    "    prediction = keras.layers.Dense(\n",
    "        units=1, activation='sigmoid',\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=keras.regularizers.l2(reg),\n",
    "        name='prediction')(pred_vector)\n",
    "    \n",
    "    aux_vector = keras.layers.Dense(\n",
    "        units=1, activation='sigmoid',\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=keras.regularizers.l2(reg),\n",
    "        name='aux_vector')(item_vector)\n",
    "    aux_vector = keras.layers.Flatten(name='aux_output')(aux_vector)\n",
    "\n",
    "    model = keras.models.Model(inputs=[user_input, item_input],\n",
    "                               outputs=[prediction, aux_vector])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(object):\n",
    "    \"\"\"Used to generate different sets of arguments\"\"\"\n",
    "    def __init__(self):\n",
    "        # default vaules\n",
    "        self.path = 'Data/'\n",
    "        self.dataset = 'ml-100k'\n",
    "        self.epochs = 20\n",
    "        self.batch_size = 256\n",
    "        self.num_tasks = 18\n",
    "        self.e_dim = 16\n",
    "        self.f_dim = 16\n",
    "        self.reg = 0\n",
    "        self.num_neg = 4\n",
    "        self.lr = 0.001\n",
    "        self.loss_weights = [1, 0.1]\n",
    "        self.K = 10\n",
    "        # self.learner = 'adam' \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = 99\n",
    "num_items = 101\n",
    "model = get_model(num_users,\n",
    "                  num_items,\n",
    "                  num_tasks=args.num_tasks,\n",
    "                  e_dim=args.e_dim,\n",
    "                  f_dim=args.f_dim,\n",
    "                  reg=args.reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "item_input (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gitem_embedding (Embedding)     (None, 1, 16)        1616        item_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "user_input (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 16)           0           gitem_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "user_embedding (Embedding)      (None, 1, 16)        1584        user_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "item_vector (Dense)             (None, 288)          4896        flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 16)           0           user_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "multitask_vector (Reshape)      (None, 18, 16)       0           item_vector[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 18)           0           flatten_2[0][0]                  \n",
      "                                                                 multitask_vector[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 16)           0           dot_2[0][0]                      \n",
      "                                                                 multitask_vector[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (Reshape)       (None, 16)           0           dot_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32)           0           flatten_2[0][0]                  \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "aux_vector (Dense)              (None, 18, 1)        17          multitask_vector[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "prediction (Dense)              (None, 1)            33          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "aux_output (Reshape)            (None, 18)           0           aux_vector[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 8,146\n",
      "Trainable params: 8,146\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
